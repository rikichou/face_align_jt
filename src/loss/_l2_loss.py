'''
@Author: Jiangtao
@Date: 2019-08-07 10:42:06
* @LastEditors  : Please set LastEditors
* @LastEditTime : 2021-10-09 16:26:56
@Description: 
'''
import os
import sys
import time


import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

from .focal_loss import FocalLoss, FocalLossWithSigmoid
from .ghm_loss import GHMC
from .wing_loss import WingLoss
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class cls_Loss_HardMining(nn.Module):

    def __init__(self,pred_for,target_for):
        super(cls_Loss_HardMining, self).__init__()
        self.pred_for = pred_for
        self.target_for = target_for
    def forward(self,pred,target):
        '''
        pred n*c    (network output)
        target n*1   (need onehot label)
        pred_index and target_index should not be equal
        '''

        # turn onehot
        if(1):
            N = pred.size(0) #batch size
            C = pred.size(1) #class num
            target = target.view((N,1))
            target_onehot = torch.FloatTensor(N, C)

            # In your for loop
            device = target.device
            target_onehot = target_onehot.to(device)

            target_onehot.zero_()
            target_onehot.scatter_(1, target.long(), 1)

            target_onehot = target_onehot.type_as(pred)
            target = target_onehot

        # find pred max and target max:N,N
        pred_index = torch.max(pred,1)[1]
        target_index = torch.max(target,1)[1]
        
        # base scale:N,C
        scale_0 = torch.ones_like(pred)
        
        # find if pred_max==pred_for,if so,put 1,else 0
        pred_0 = pred_index == self.pred_for
        tartget_1 = target_index == self.target_for

        # add pred_0 and target_1.some will equal 2,if so,put 1,else 0.
        scale_1 = (pred_0 + tartget_1).type_as(scale_0)
        scale_1 = torch.gt(scale_1,torch.ones_like(scale_1))

        # resize to N,C
        scale_1 = scale_1.reshape((-1,1)).expand_as(pred).type_as(scale_0)

        # scale = scale0 + 1:N,C
        scale = scale_0+scale_1

        # loss:N,C scale:N,C
        loss = torch.nn.functional.binary_cross_entropy_with_logits(pred, target, reduction = 'none')
        loss = loss * scale
        loss = loss.sum()

        return loss
        
class mse_Loss(nn.Module):
    
    def __init__(self,deviceId):
        super(mse_Loss, self).__init__()

        self.mse = nn.MSELoss(reduction='none')
        self.mse = self.mse.to(torch.device("cuda:{}".format(deviceId)))

    def forward(self, output, target):

        loss = 0
        # 眼睛
        target = target.view_as(output).float()
        loss += self.mse(output, target).sum()

        # 损失均值化
        loss = loss / float(output.size()[0])
    
        return loss

class cls_Loss(nn.Module):
    
    def __init__(self,deviceId):
        super(cls_Loss, self).__init__()

        self.gpu_id = deviceId
        self.CE = nn.CrossEntropyLoss()
        self.CE = self.CE.to(torch.device("cuda:{}".format(deviceId)))
        self.FocalLoss = FocalLoss()
        self.FocalLoss_multiLabel = FocalLoss(classes=2)
        self.MultiLabelSoftMarginLoss = nn.MultiLabelSoftMarginLoss()

    def forward(self, output, target):
        '''
        output n,c
        target n
        '''
        # print(type(target))
        target = target.float()
        target = Variable(target,requires_grad=True).cuda(self.gpu_id).long()
        # print(output.shape)
        # print(target.shape)
        loss = 0
        
        if(0):
            loss += self.FocalLoss(output, target)
        if(1):
            loss += self.CE(output, target.squeeze())
        if(0):
            loss += self.GHMloss(output, target)
        if(0):
            if(1):
                N = output.size(0) #batch size
                C = output.size(1) #class num
                target = target.view((N,1))
                target_onehot = torch.FloatTensor(N, C)

                # In your for loop
                device = target.device
                target_onehot = target_onehot.to(device)

                target_onehot.zero_()
                target_onehot.scatter_(1, target, 1)

                target_onehot = target_onehot.type_as(output)
                target = target_onehot
                
            loss += self.MultiLabelSoftMarginLoss(output, target)
            
        # 损失均值化
        loss = loss / float(output.size()[0])

        return loss

class mse_Loss_optimize_mask(nn.Module):
    
    def __init__(self,deviceId):
        super(mse_Loss_optimize_mask, self).__init__()

        self.mse = nn.MSELoss(reduction='none')
        self.mse = self.mse.to(torch.device("cuda:{}".format(deviceId)))

    def forward(self, output, target):

        loss = 0
        # 眼睛
        target = target.view_as(output).float()
        loss += self.mse(output, target)
        a= loss > loss.mean()
        loss = loss[a]

        # 损失均值化
        loss = loss.sum() / float(output.size()[0])
    
        return loss

class mse_Loss_optimize_angle(nn.Module):
    
    def __init__(self,deviceId):
        super(mse_Loss_optimize_angle, self).__init__()

        self.mse = nn.MSELoss(reduction='none')
        self.mse = self.mse.to(torch.device("cuda:{}".format(deviceId)))

    def forward(self, output, target):

        loss = 0

        # (B,22),calculate loss
        target = target.view_as(output).float()
        loss += self.mse(output, target)

        # (B),calculate 2 scale and merge them to one
        if(22 == target.shape[1]):
            left = target[:,0]
            mid = target[:,10]
            right = target[:,20]
            
        if(26 == target.shape[1]):
            left = target[:,0]
            mid = target[:,12]
            right = target[:,24]


        
        abs = torch.abs
        scale1 = abs((mid-left) / ((right-mid)+1e-5)) 
        scale2 = abs((right-mid) / ((mid-left)+1e-5))

        scale1 = torch.where(scale1 < 0, torch.full_like(scale1, 1), scale1)
        scale2 = torch.where(scale2 < 0, torch.full_like(scale2, 1), scale2)
        scale1 = torch.where(scale1 > 100, torch.full_like(scale1, 100), scale1)
        scale2 = torch.where(scale2 > 100, torch.full_like(scale2, 100), scale2)

        scale = torch.where(scale1>scale2,scale1,scale2)
        scale = torch.sqrt(scale+1e-5)
        # scale = torch.log(scale)
        
        loss = torch.sum(loss,dim=1)
        loss *= scale

        # 损失均值化
        loss = loss.sum() / float(output.size()[0])
    
        return loss

class mse_Loss_optimize_angle_mouth(nn.Module):
    
    def __init__(self,deviceId):
        super(mse_Loss_optimize_angle_mouth, self).__init__()

        self.mse = nn.MSELoss(reduction='none')
        self.mse = self.mse.to(torch.device("cuda:{}".format(deviceId)))

    def forward(self, output, target, eyeGt):

        loss = 0

        # (B,22),calculate loss
        target = target.view_as(output).float()
        loss += self.mse(output, target)

        # (B),calculate 2 scale and merge them to one
        eyeGt = eyeGt.view((target.shape[0],-1)).float()
        left = eyeGt[:,0]
        mid = eyeGt[:,10]
        right = eyeGt[:,20]
        
        abs = torch.abs
        scale1 = abs((mid-left) / ((right-mid)+1e-5)) 
        scale2 = abs((right-mid) / ((mid-left)+1e-5))

        scale1 = torch.where(scale1 < 0, torch.full_like(scale1, 1), scale1)
        scale2 = torch.where(scale2 < 0, torch.full_like(scale2, 1), scale2)
        scale1 = torch.where(scale1 > 100, torch.full_like(scale1, 100), scale1)
        scale2 = torch.where(scale2 > 100, torch.full_like(scale2, 100), scale2)

        scale = torch.where(scale1>scale2,scale1,scale2)
        scale = torch.sqrt(scale+1e-5)
        # scale = torch.log(scale)
        
        loss = torch.sum(loss,dim=1)
        loss *= scale

        # 损失均值化
        loss = loss.sum() / float(output.size()[0])
    
        return loss